{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Notebook\n",
    "\n",
    "Dear User,\n",
    "\n",
    "this Jupyter Notebook contains the code and AI model to our challenge \"Rare Diseases\" by the LMU childrens hospital.\n",
    "Here our various functions come together to create a data pipeline and readable as well as modular model architecture.\n",
    "There are few medical terms being used whose understanding is necessary and they are the following:\n",
    "\n",
    "- `HPO Features`: Human Phenotype Ontology Features provide a standardized vocabulary of phenotypic abnormalities encountered in human disease.\n",
    "- `ICD-9`: International standard for the classification of diseases\n",
    "\n",
    "Most parameters used to tune training are found in the [Training Parameters](#training-parameters) section\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nn_data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import nn_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Labevents_HPO are the HPO features which where measured during an examination of a patient\n",
    "# Diagnoses_HPO are the HPO features that were diagnosed based\n",
    "HPO_PATH = 'data/hp.obo'\n",
    "LABEVENTS_HPO_PATH = 'data/OUT_LABEVENTS_HPO.csv'\n",
    "DIAGNOSES_HPO_PATH = 'data/DIAGNOSE_ICD_hpo.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data (takes about 4 seconds)\n",
    "\n",
    "If the data format is different, use `labevents_hpo_column_name` and `diagnoses_hpo_column_name` parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads HPO data, labevents and diagnoses and groups them by subject ID\n",
    "data = nn_data.LoadedData(HPO_PATH, LABEVENTS_HPO_PATH, DIAGNOSES_HPO_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_ICD: bool = False\n",
    "enable_parent_nodes_hpo_input: bool = False\n",
    "enable_parent_nodes_hpo_target: bool = False # only used if `use_ICD == False`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_autoencoder: bool = True\n",
    "\n",
    "# set the parameters for the training of the autoencoder\n",
    "batch_size_AE: int = 8\n",
    "num_epochs_AE: int = 60\n",
    "learning_rate_AE: float = 1e-2\n",
    "beta_AE: tuple[float, float] = (0.9, 0.999)\n",
    "\n",
    "# layer sizes for the autoencoder\n",
    "reduction_factor_hidden: float = 0.7\n",
    "reduction_factor_latent: float = 0.5\n",
    "\n",
    "# best loss_function determined experimentally\n",
    "loss_func_AE: nn.Module = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for Main Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_autoencoder:\n",
    "    batch_size: int = 8\n",
    "    num_epochs: int = 20\n",
    "    learning_rate: float = 1e-4\n",
    "    betas: tuple[float, float] = (0.9, 0.999)\n",
    "    enlarging_factor_NN: float = 1.4\n",
    "    dropOutRatio: float = 0.0\n",
    "\n",
    "    # best loss_function determined experimentally\n",
    "    loss_func_NN: nn.Module = nn.CrossEntropyLoss()\n",
    "else:\n",
    "    batch_size: int = 8\n",
    "    num_epochs: int = 20\n",
    "    learning_rate: float = 1e-4\n",
    "    betas: tuple[float, float] = (0.9, 0.999)\n",
    "    enlarging_factor_NN: float = 1.4\n",
    "    dropOutRatio: float = 0.0\n",
    "\n",
    "    # best loss_function determined experimentally\n",
    "    loss_func_NN: nn.Module = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits the loaded data in labevents and diagnoses\n",
    "input_data_creator = nn_data.HPODatasetCreator(\n",
    "    data, 'labevents',\n",
    "    enable_parent_nodes=enable_parent_nodes_hpo_input,\n",
    ")\n",
    "if use_ICD:\n",
    "    target_data_creator = nn_data.ICDDatasetCreator(data, batch=True)\n",
    "else:\n",
    "    target_data_creator = nn_data.HPODatasetCreator(\n",
    "        data, 'diagnoses',\n",
    "        enable_parent_nodes=enable_parent_nodes_hpo_target,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data from the input and target_data_creators are transferred in a nested list structure\n",
    "input_data: list[list[int]] = input_data_creator.data()\n",
    "target_data: list[list[int]] = target_data_creator.data()\n",
    "\n",
    "# for pytorch the list structure is transformed in tensors\n",
    "input_tensor = torch.FloatTensor(input_data)\n",
    "target_tensor = torch.FloatTensor(target_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(output, target) -> float:\n",
    "    \"\"\"\n",
    "    computes the portion of correctly predicted outputs\n",
    "    \"\"\"\n",
    "    number_of_features = target.sum(axis=1)\n",
    "    correctly_identified = (target * output).sum(axis=1)\n",
    "    return np.mean(correctly_identified / (number_of_features + .00001))\n",
    "    #  + .00001 avoid divided by zero errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_real_effect(outputs, targets):\n",
    "    \"\"\"\n",
    "    lists the correctly identified diagnoses, false positives and false negatives\n",
    "    correct diagnoses counts only recognized active features and not neglected inactive features\n",
    "    \"\"\" \n",
    "    correct_diagnosed = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    total_to_diagnose = sum(targets[0])\n",
    "\n",
    "    for i in range(len(outputs[0])):\n",
    "        if(outputs[0, i] >= 0.5 and targets[0, i] == 1):\n",
    "            correct_diagnosed += 1\n",
    "        if(outputs[0, i] < 0.5 and targets[0, i] == 1):\n",
    "            false_negative += 1\n",
    "        if(outputs[0, i] > 0.5 and targets[0, i] == 0):\n",
    "            false_positive += 1\n",
    "\n",
    "    print(\"Correct diagnoses:\" f'{correct_diagnosed}/{total_to_diagnose}')\n",
    "    print(\"False positives:\" f'{false_positive}')\n",
    "    print(\"False negatives:\" f'{false_negative}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device selection, where NN is trained\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_autoencoder:\n",
    "    # Dimensions of the Autoencoder\n",
    "    input_size_AE = len(input_data[0])\n",
    "    hidden_size_AE = int(input_size_AE*reduction_factor_hidden)\n",
    "    latent_size_AE = int(input_size_AE*reduction_factor_latent)\n",
    "\n",
    "    # In Autoencoder function the architecture is built\n",
    "    AE = nn_files.Autoencoder(input_size_AE, hidden_size_AE, latent_size_AE)\n",
    "    AE.to(device)\n",
    "\n",
    "    # Latent space of autoencoder is used as input for the FCN\n",
    "    input_size_NN = latent_size_AE\n",
    "else:\n",
    "    AE = None\n",
    "\n",
    "    input_size_NN = len(input_data[0])\n",
    "\n",
    "output_size_NN = len(target_data[0])\n",
    "hidden_size_NN = int(max(input_size_NN, output_size_NN) * enlarging_factor_NN)\n",
    "\n",
    "# Call of FCNModel function, can build Model differently depending on if encoder is used or not\n",
    "model = nn_files.FCNModel(input_size_NN, hidden_size_NN,\n",
    "                          output_size_NN, enlarging_factor_NN, AE, dropOutRatio=dropOutRatio)\n",
    "                          \n",
    "_ = model.to(device) # `_ =` is used to suppress jupyter output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Autoencoder (if enabled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if use_autoencoder:\n",
    "    assert AE is not None  # the linter is not smart enough to infer that\n",
    "    # Bestimmung des Optimizers, standard: Adam\n",
    "    optimizer_AE = torch.optim.Adam(\n",
    "        AE.parameters(),\n",
    "        lr=learning_rate_AE,\n",
    "        betas=beta_AE,\n",
    "    )\n",
    "\n",
    "    # create separate dataset for Autoencoder, as output of model is not compared to original target\n",
    "    # but again to the input\n",
    "    dataset_AE = torch.utils.data.TensorDataset(input_tensor, input_tensor)\n",
    "    # split the dataset in 70% training data, 20% validation data, 10% test data\n",
    "    dataset_AE_split = nn_files.split_dataset(batch_size_AE, dataset_AE)\n",
    "\n",
    "    # train autoencoder with general training function, can handle autoencoder and other models\n",
    "    nn_files.training(\n",
    "        AE, device, dataset_AE_split,\n",
    "        optimizer=optimizer_AE, loss_func=loss_func_AE,  num_epochs=num_epochs_AE,\n",
    "    )\n",
    "else:\n",
    "    dataset_AE_split = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_autoencoder:\n",
    "    assert AE is not None and dataset_AE_split is not None\n",
    "    # the linter is not smart enough to infer that\n",
    "    \n",
    "    # show the output of the autoencoder to visualize ability to decode and reconstruct multi-hot encoding vectors\n",
    "    # X visualize an active feature, blanks a zero\n",
    "    nn_files.test(\n",
    "        AE, device, dataset_AE_split, data_creator=input_data_creator,\n",
    "        plot_outputs=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Main Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bestimmung des Optimizers, standard: Adam\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    betas=betas,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset in which the input and target aka ground truth tensor are located next to each other\n",
    "dataset = torch.utils.data.TensorDataset(input_tensor, target_tensor)\n",
    "# split the dataset in 70% training data, 20% validation data, 10% test data\n",
    "dataset_split = nn_files.split_dataset(batch_size, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model, log_rhythm denotes after how many iterations in one epoch \n",
    "# the loss and accuracy if applicable is print\n",
    "nn_files.training(\n",
    "    model, device, dataset_split,\n",
    "    num_epochs=num_epochs,\n",
    "    optimizer=optimizer, loss_func=loss_func_NN,\n",
    "    calc_accuracy=calc_accuracy,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the real effect of the prediction, aka understandable by humans in a way of classification in\n",
    "# correct diagnoses, false positives and false negatives\n",
    "nn_files.test(\n",
    "    model, device, dataset_split,\n",
    "    target_data_creator,\n",
    "    print_real_effect=print_real_effect, calc_accuracy=calc_accuracy,\n",
    "    plot_outputs=True, plot_decide=True,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "daceadcb7888934c6f7a76168830572155d751a6b865934a713a3e2629d639d6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
