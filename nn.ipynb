{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nn_data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import nn_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HPO_PATH = 'data/hp.obo'\n",
    "LABEVENTS_HPO_PATH = 'data/OUT_LABEVENTS_HPO.csv'\n",
    "DIAGNOSES_HPO_PATH = 'data/DIAGNOSE_ICD_hpo.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = nn_data.LoadedData(HPO_PATH, LABEVENTS_HPO_PATH, DIAGNOSES_HPO_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_creator = nn_data.HPODatasetCreator(data, mode='labevents', enable_parent_nodes=False)\n",
    "target_data_creator = nn_data.HPODatasetCreator(data, mode='diagnoses', enable_parent_nodes=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data: list[list[int]] = input_data_creator.data()\n",
    "target_data: list[list[int]] = target_data_creator.data()\n",
    "\n",
    "input_tensor = torch.FloatTensor(input_data)\n",
    "target_tensor = torch.FloatTensor(target_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of accuracy function\n",
    "def calc_accuracy(output, target) -> float:\n",
    "    result = np.zeros(output.shape)\n",
    "\n",
    "    number_of_features = target.sum(axis=1)\n",
    "    correctly_identified = (target * np.sqrt(output)).sum(axis=1)\n",
    "    return np.mean(correctly_identified / (number_of_features + .00001))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of real effect function\n",
    "def real_effect(outputs, targets):\n",
    "    correct_diagnosed = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    total_to_diagnose = sum(targets[0])\n",
    "\n",
    "    for i in range(len(outputs[0])):\n",
    "        if(outputs[0, i] >= 0.5 and targets[0, i] == 1):\n",
    "            correct_diagnosed += 1\n",
    "        if(outputs[0, i] < 0.5 and targets[0, i] == 1):\n",
    "            false_negative += 1\n",
    "        if(outputs[0, i] > 0.5 and targets[0, i] == 0):\n",
    "            false_positive += 1\n",
    "\n",
    "    print(\"Correct diagnoses:\" f'{correct_diagnosed}/{total_to_diagnose}')\n",
    "    print(\"False positives:\" f'{false_positive}')\n",
    "    print(\"False negatives:\" f'{false_negative}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of loss function\n",
    "loss_func_nn = nn.CrossEntropyLoss()\n",
    "loss_func_auto = nn.MSELoss()\n",
    "# usefullness od CrossEntropyLoss determined experimentally, best among available pytorch loss functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage of Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ste True or False\n",
    "use_autoencoder = False\n",
    "\n",
    "# device selection, where NN is trained\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_autoencoder:\n",
    "    # Model of the AutoEncoder\n",
    "    reduction_factor_hidden = 0.7\n",
    "    reduction_factor_latent = 0.5\n",
    "\n",
    "    input_size_AE = len(input_data[0])\n",
    "    hidden_size_AE = int(input_size_AE*reduction_factor_hidden)\n",
    "    latent_size_AE = int(input_size_AE*reduction_factor_latent)\n",
    "    \n",
    "    AE = nn_files.Autoencoder(input_size_AE, hidden_size_AE, latent_size_AE)\n",
    "    \n",
    "    AE.to(device)\n",
    "    \n",
    "    # Model of NN with Encoder Structure\n",
    "    enlarging_factor_NN = 1.4\n",
    "    input_size_NN = latent_size_AE\n",
    "else:\n",
    "    AE = None\n",
    "    \n",
    "    # Model of NN without Encoder Structure\n",
    "    enlarging_factor_NN = 1.4\n",
    "    input_size_NN = len(input_data[0])\n",
    "    \n",
    "output_size_NN = len(target_data[0])\n",
    "hidden_size_NN = int(max(input_size_NN, output_size_NN) * enlarging_factor_NN)\n",
    "\n",
    "model = nn_files.NN(input_size_NN, hidden_size_NN, output_size_NN, enlarging_factor_NN, AE)\n",
    "_ = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with Training Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if use_autoencoder:\n",
    "    \n",
    "    batch_size = 8\n",
    "    learning_rate=1e-2\n",
    "    num_epochs = 60\n",
    "    \n",
    "    # optimizer\n",
    "    # Bestimmung des Optimizers, standard: Adam\n",
    "    optimizer = torch.optim.Adam(\n",
    "        AE.parameters(),\n",
    "        lr=learning_rate,\n",
    "        betas=(0.9, 0.999),\n",
    "    )\n",
    "    \n",
    "    dataset_AE = torch.utils.data.TensorDataset(input_tensor, input_tensor)\n",
    "    dataset_AE_split = nn_files.split_dataset(batch_size, dataset_AE)\n",
    "    \n",
    "    # train autoencoder\n",
    "    nn_files.training(model = AE, optimizer=optimizer, loss_func= loss_func_auto, dataset_split=dataset_AE_split,\n",
    "                     learning_rate=learning_rate, num_epochs=num_epochs, device=device, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_autoencoder:\n",
    "    nn_files.test(model = AE,loss_func=loss_func_nn, device=device, dataset_split =dataset_AE_split,\n",
    "              target_data_creator=target_data_creator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_autoencoder:\n",
    "    batch_size = 8\n",
    "    learning_rate=1e-4\n",
    "    num_epochs = 20\n",
    "    \n",
    "    # optimizer\n",
    "    # Bestimmung des Optimizers, standard: Adam\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        betas=(0.9, 0.999),\n",
    "    )\n",
    "    \n",
    "    # train NN with encoder structure\n",
    "else:\n",
    "    batch_size = 8\n",
    "    learning_rate=1e-4\n",
    "    num_epochs = 20\n",
    "    \n",
    "    # optimizer\n",
    "    # Bestimmung des Optimizers, standard: Adam\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        betas=(0.9, 0.999),\n",
    "    )\n",
    "    \n",
    "    # train NN without encoder Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.utils.data.TensorDataset(input_tensor, target_tensor)\n",
    "dataset_split = nn_files.split_dataset(batch_size, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/20]\n",
      "[Iteration 5]\tTRAIN      loss/acc: 157.105\t0.715\n",
      "for this epoch:\tTRAIN      loss/acc: 144.575\t0.709\n",
      "\t\tVALIDATION loss/acc: 136.583\t0.705\n",
      "Correct diagnoses:5/5.0\n",
      "False positives:212\n",
      "False negatives:0\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 2/20]\n",
      "[Iteration 5]\tTRAIN      loss/acc: 145.009\t0.731\n",
      "for this epoch:\tTRAIN      loss/acc: 145.405\t0.751\n",
      "\t\tVALIDATION loss/acc: 135.572\t0.735\n",
      "Correct diagnoses:5/5.0\n",
      "False positives:188\n",
      "False negatives:0\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 3/20]\n",
      "[Iteration 5]\tTRAIN      loss/acc: 135.767\t0.771\n",
      "for this epoch:\tTRAIN      loss/acc: 147.656\t0.781\n",
      "\t\tVALIDATION loss/acc: 134.559\t0.757\n",
      "Correct diagnoses:5/5.0\n",
      "False positives:159\n",
      "False negatives:0\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 4/20]\n",
      "[Iteration 5]\tTRAIN      loss/acc: 137.609\t0.798\n",
      "for this epoch:\tTRAIN      loss/acc: 140.176\t0.809\n",
      "\t\tVALIDATION loss/acc: 133.660\t0.772\n",
      "Correct diagnoses:5/5.0\n",
      "False positives:136\n",
      "False negatives:0\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 5/20]\n",
      "[Iteration 5]\tTRAIN      loss/acc: 161.299\t0.792\n",
      "for this epoch:\tTRAIN      loss/acc: 137.658\t0.829\n",
      "\t\tVALIDATION loss/acc: 132.836\t0.778\n",
      "Correct diagnoses:5/5.0\n",
      "False positives:115\n",
      "False negatives:0\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 6/20]\n",
      "[Iteration 5]\tTRAIN      loss/acc: 159.391\t0.849\n",
      "for this epoch:\tTRAIN      loss/acc: 138.960\t0.843\n",
      "\t\tVALIDATION loss/acc: 132.104\t0.779\n",
      "Correct diagnoses:5/5.0\n",
      "False positives:104\n",
      "False negatives:0\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 7/20]\n",
      "[Iteration 5]\tTRAIN      loss/acc: 127.675\t0.843\n",
      "for this epoch:\tTRAIN      loss/acc: 143.685\t0.850\n",
      "\t\tVALIDATION loss/acc: 131.521\t0.775\n",
      "Correct diagnoses:5/5.0\n",
      "False positives:93\n",
      "False negatives:0\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 8/20]\n",
      "[Iteration 5]\tTRAIN      loss/acc: 142.357\t0.848\n",
      "for this epoch:\tTRAIN      loss/acc: 137.306\t0.855\n",
      "\t\tVALIDATION loss/acc: 131.081\t0.769\n",
      "Correct diagnoses:5/5.0\n",
      "False positives:93\n",
      "False negatives:0\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 9/20]\n",
      "[Iteration 5]\tTRAIN      loss/acc: 126.312\t0.831\n",
      "for this epoch:\tTRAIN      loss/acc: 138.498\t0.858\n",
      "\t\tVALIDATION loss/acc: 130.787\t0.763\n",
      "Correct diagnoses:5/5.0\n",
      "False positives:93\n",
      "False negatives:0\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 10/20]\n",
      "[Iteration 5]\tTRAIN      loss/acc: 128.353\t0.842\n",
      "for this epoch:\tTRAIN      loss/acc: 140.502\t0.855\n",
      "\t\tVALIDATION loss/acc: 130.559\t0.757\n",
      "Correct diagnoses:5/5.0\n",
      "False positives:92\n",
      "False negatives:0\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 11/20]\n",
      "[Iteration 5]\tTRAIN      loss/acc: 143.958\t0.861\n",
      "for this epoch:\tTRAIN      loss/acc: 139.773\t0.855\n",
      "\t\tVALIDATION loss/acc: 130.362\t0.752\n",
      "Correct diagnoses:5/5.0\n",
      "False positives:93\n",
      "False negatives:0\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 12/20]\n",
      "[Iteration 5]\tTRAIN      loss/acc: 143.129\t0.858\n",
      "for this epoch:\tTRAIN      loss/acc: 137.565\t0.854\n",
      "\t\tVALIDATION loss/acc: 130.242\t0.748\n",
      "Correct diagnoses:5/5.0\n",
      "False positives:91\n",
      "False negatives:0\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 13/20]\n",
      "[Iteration 5]\tTRAIN      loss/acc: 128.318\t0.819\n",
      "for this epoch:\tTRAIN      loss/acc: 139.571\t0.852\n",
      "\t\tVALIDATION loss/acc: 130.139\t0.745\n",
      "Correct diagnoses:5/5.0\n",
      "False positives:90\n",
      "False negatives:0\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 14/20]\n",
      "[Iteration 5]\tTRAIN      loss/acc: 152.609\t0.862\n",
      "for this epoch:\tTRAIN      loss/acc: 135.770\t0.852\n",
      "\t\tVALIDATION loss/acc: 130.074\t0.742\n",
      "Correct diagnoses:5/5.0\n",
      "False positives:90\n",
      "False negatives:0\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 15/20]\n",
      "[Iteration 5]\tTRAIN      loss/acc: 100.737\t0.825\n",
      "for this epoch:\tTRAIN      loss/acc: 139.592\t0.850\n",
      "\t\tVALIDATION loss/acc: 130.008\t0.739\n",
      "Correct diagnoses:5/5.0\n",
      "False positives:88\n",
      "False negatives:0\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 16/20]\n",
      "[Iteration 5]\tTRAIN      loss/acc: 130.023\t0.881\n",
      "for this epoch:\tTRAIN      loss/acc: 134.673\t0.845\n",
      "\t\tVALIDATION loss/acc: 129.933\t0.736\n",
      "Correct diagnoses:5/5.0\n",
      "False positives:89\n",
      "False negatives:0\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 17/20]\n",
      "[Iteration 5]\tTRAIN      loss/acc: 127.079\t0.854\n",
      "for this epoch:\tTRAIN      loss/acc: 137.178\t0.845\n",
      "\t\tVALIDATION loss/acc: 129.890\t0.734\n",
      "Correct diagnoses:5/5.0\n",
      "False positives:89\n",
      "False negatives:0\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 18/20]\n",
      "[Iteration 5]\tTRAIN      loss/acc: 137.723\t0.854\n",
      "for this epoch:\tTRAIN      loss/acc: 136.697\t0.840\n",
      "\t\tVALIDATION loss/acc: 129.865\t0.732\n",
      "Correct diagnoses:5/5.0\n",
      "False positives:88\n",
      "False negatives:0\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 19/20]\n",
      "[Iteration 5]\tTRAIN      loss/acc: 124.508\t0.864\n",
      "for this epoch:\tTRAIN      loss/acc: 134.687\t0.842\n",
      "\t\tVALIDATION loss/acc: 129.864\t0.730\n",
      "Correct diagnoses:5/5.0\n",
      "False positives:87\n",
      "False negatives:0\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 20/20]\n",
      "[Iteration 5]\tTRAIN      loss/acc: 141.973\t0.858\n",
      "for this epoch:\tTRAIN      loss/acc: 133.924\t0.845\n",
      "\t\tVALIDATION loss/acc: 129.840\t0.729\n",
      "Correct diagnoses:5/5.0\n",
      "False positives:87\n",
      "False negatives:0\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_files.training(model = model, optimizer=optimizer, loss_func=loss_func_nn, batch_size=batch_size,\n",
    "                  dataset_split=dataset_split,\n",
    "                 learning_rate=learning_rate, num_epochs=num_epochs, device=device, real_effect=real_effect, \n",
    "                 log_rhythm = 5, calc_accuracy = calc_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct diagnoses:29/38.0\n",
      "False positives:69\n",
      "False negatives:9\n",
      "\n",
      "Correct diagnoses:6/7.0\n",
      "False positives:90\n",
      "False negatives:1\n",
      "\n",
      "Correct diagnoses:15/21.0\n",
      "False positives:80\n",
      "False negatives:6\n",
      "\n",
      "Correct diagnoses:1/1.0\n",
      "False positives:93\n",
      "False negatives:0\n",
      "\n",
      "Correct diagnoses:0/0.0\n",
      "False positives:93\n",
      "False negatives:0\n",
      "\n",
      "Correct diagnoses:7/8.0\n",
      "False positives:89\n",
      "False negatives:1\n",
      "\n",
      "Correct diagnoses:7/19.0\n",
      "False positives:92\n",
      "False negatives:12\n",
      "\n",
      "Correct diagnoses:2/9.0\n",
      "False positives:94\n",
      "False negatives:7\n",
      "\n",
      "Correct diagnoses:3/4.0\n",
      "False positives:94\n",
      "False negatives:1\n",
      "\n",
      "Correct diagnoses:3/38.0\n",
      "False positives:89\n",
      "False negatives:35\n",
      "\n",
      "Test Accuracy: 0.627\n",
      "HP:0002621\t0.27\t \n",
      "HP:0000112\t0.05\t \n",
      "HP:0001251\t0.13\t \n",
      "HP:0000601\t0.07\t \n",
      "HP:0001335\t0.07\t \n",
      "HP:0002224\t0.17\t \n",
      "HP:0000407\t0.08\t \n",
      "HP:0000490\t0.06\t \n",
      "HP:0002929\t0.12\t \n",
      "HP:0004388\t0.13\t \n",
      "HP:0006698\t0.17\t \n",
      "HP:0002015\t0.36\t \n",
      "HP:0004409\t0.38\t \n",
      "HP:0000658\t0.17\t \n",
      "HP:0000270\t0.08\t \n",
      "HP:0001674\t0.06\tX\n",
      "HP:0008213\t0.09\t \n",
      "HP:0004510\t0.06\t \n",
      "HP:0011462\t0.23\t \n",
      "HP:0012407\t0.24\t \n",
      "HP:0004942\t0.67\t \n",
      "HP:0001644\t0.23\t \n",
      "HP:0000815\t0.11\t \n",
      "HP:0031677\t0.53\t \n",
      "HP:0002595\t0.10\t \n",
      "HP:0000982\t0.21\t \n",
      "HP:0011009\t0.37\t \n",
      "HP:0100257\t0.10\t \n",
      "HP:0004308\t0.87\t \n",
      "HP:0025267\t0.12\t \n",
      "HP:0007302\t0.10\t \n",
      "HP:0001355\t0.21\t \n",
      "HP:0002380\t0.09\t \n",
      "HP:0000123\t0.06\t \n",
      "HP:0001250\t0.79\t \n",
      "HP:0002664\t0.49\t \n",
      "HP:0001993\t0.95\t \n",
      "HP:0005293\t0.40\t \n",
      "HP:0000870\t0.26\t \n",
      "HP:0006767\t0.22\t \n",
      "HP:0005616\t0.11\t \n",
      "HP:0001520\t0.04\t \n",
      "HP:0000709\t0.84\t \n",
      "HP:0005206\t0.10\t \n",
      "HP:0012251\t0.61\t \n",
      "HP:0002555\t0.10\t \n",
      "HP:0000175\t0.12\t \n",
      "HP:0008850\t0.11\t \n",
      "HP:0011705\t0.53\t \n",
      "HP:0002097\t0.14\t \n",
      "HP:0001256\t0.07\t \n",
      "HP:0001737\t0.10\t \n",
      "HP:0002588\t0.09\t \n",
      "HP:0001288\t0.19\t \n",
      "HP:0002329\t0.27\t \n",
      "HP:0000961\t0.08\t \n",
      "HP:0001249\t0.52\tX\n",
      "HP:0000028\t0.08\t \n",
      "HP:0008151\t0.30\t \n",
      "HP:0003477\t0.06\t \n",
      "HP:0004220\t0.08\tX\n",
      "HP:0002866\t0.05\tX\n",
      "HP:0003077\t0.94\t \n",
      "HP:0031079\t0.10\t \n",
      "HP:0100786\t0.10\t \n",
      "HP:0006510\t0.87\t \n",
      "HP:0000505\t0.09\t \n",
      "HP:0005990\t0.09\t \n",
      "HP:0008214\t0.12\t \n",
      "HP:0000952\t0.08\t \n",
      "HP:0001394\t0.06\t \n",
      "HP:0011560\t0.64\t \n",
      "HP:0002172\t0.17\t \n",
      "HP:0000031\t0.08\t \n",
      "HP:0000508\t0.04\t \n",
      "HP:0003593\t0.42\t \n",
      "HP:0002910\t0.08\t \n",
      "HP:0012838\t0.12\t \n",
      "HP:0001287\t0.13\t \n",
      "HP:0001892\t0.19\t \n",
      "HP:0001513\t0.24\t \n",
      "HP:0000939\t0.92\t \n",
      "HP:0001342\t0.38\t \n",
      "HP:0005110\t0.94\t \n",
      "HP:0031273\t0.73\t \n",
      "HP:0000824\t0.08\t \n",
      "HP:0000771\t0.13\t \n",
      "HP:0025401\t0.26\t \n",
      "HP:0001871\t0.04\t \n",
      "HP:0040075\t0.13\t \n",
      "HP:0001712\t0.68\t \n",
      "HP:0004418\t0.08\t \n",
      "HP:0032070\t0.06\t \n",
      "HP:0003196\t0.11\t \n",
      "HP:0004398\t0.23\t \n",
      "HP:0000204\t0.09\t \n",
      "HP:0001762\t0.18\t \n",
      "HP:0002789\t0.11\t \n",
      "HP:0001723\t0.20\t \n",
      "HP:0012302\t0.07\t \n",
      "HP:0007311\t0.16\t \n",
      "HP:0002758\t0.24\t \n",
      "HP:0001370\t0.05\t \n",
      "HP:0003510\t0.13\t \n",
      "HP:0001278\t0.21\t \n",
      "HP:0002353\t0.22\t \n",
      "HP:0001733\t0.14\t \n",
      "HP:0008734\t0.13\t \n",
      "HP:0001688\t0.87\t \n",
      "HP:0000846\t0.07\t \n",
      "HP:0000716\t0.90\t \n",
      "HP:0002086\t0.05\t \n",
      "HP:0031819\t0.96\t \n",
      "HP:0001259\t0.49\t \n",
      "HP:0005280\t0.13\t \n",
      "HP:0009830\t0.97\t \n",
      "HP:0000389\t0.13\t \n",
      "HP:0001348\t0.21\t \n",
      "HP:0001347\t0.23\t \n",
      "HP:0004755\t0.16\t \n",
      "HP:0002457\t0.14\t \n",
      "HP:0001695\t0.93\tX\n",
      "HP:0010535\t0.16\t \n",
      "HP:0002202\t0.14\t \n",
      "HP:0005528\t0.12\t \n",
      "HP:0007841\t0.04\t \n",
      "HP:0000024\t0.10\t \n",
      "HP:0001854\t0.30\tX\n",
      "HP:0006682\t0.59\t \n",
      "HP:0004395\t0.38\t \n",
      "HP:0001263\t0.38\t \n",
      "HP:0004749\t0.65\t \n",
      "HP:0003095\t0.13\t \n",
      "HP:0003799\t0.11\t \n",
      "HP:0005978\t0.96\t \n",
      "HP:0008169\t0.19\t \n",
      "HP:0010446\t0.42\t \n",
      "HP:0003581\t0.24\t \n",
      "HP:0003040\t0.09\t \n",
      "HP:0003141\t0.93\t \n",
      "HP:0001332\t0.18\t \n",
      "HP:0000135\t0.11\t \n",
      "HP:0001761\t0.10\t \n",
      "HP:0410291\t0.18\t \n",
      "HP:0000218\t0.11\t \n",
      "HP:0002743\t0.07\t \n",
      "HP:0001367\t0.11\t \n",
      "HP:0030260\t0.11\t \n",
      "HP:0000746\t0.37\t \n",
      "HP:0001657\t0.73\t \n",
      "HP:0010627\t0.12\t \n",
      "HP:0001088\t0.08\tX\n",
      "HP:0002511\t0.23\tX\n",
      "HP:0000470\t0.07\t \n",
      "HP:0006562\t0.14\t \n",
      "HP:0000248\t0.03\tX\n",
      "HP:0002360\t0.19\t \n",
      "HP:0002067\t0.22\t \n",
      "HP:0011010\t0.84\t \n",
      "HP:0003474\t0.14\t \n",
      "HP:0008480\t0.06\t \n",
      "HP:0003182\t0.07\tX\n",
      "HP:0010808\t0.05\tX\n",
      "HP:0001873\t0.07\t \n",
      "HP:0003596\t0.23\t \n",
      "HP:0002018\t0.78\t \n",
      "HP:0000021\t0.09\t \n",
      "HP:0012622\t0.06\t \n",
      "HP:0002960\t0.94\t \n",
      "HP:0025478\t0.51\t \n",
      "HP:0012378\t0.91\t \n",
      "HP:0001289\t0.10\t \n",
      "HP:0002644\t0.27\t \n",
      "HP:0003577\t0.29\t \n",
      "HP:0009741\t0.90\t \n",
      "HP:0001388\t0.05\tX\n",
      "HP:0003678\t0.14\t \n",
      "HP:0000103\t0.95\t \n",
      "HP:0031908\t0.20\t \n",
      "HP:0000405\t0.07\tX\n",
      "HP:0000126\t0.08\t \n",
      "HP:0003701\t0.46\t \n",
      "HP:0012115\t0.09\t \n",
      "HP:0030646\t0.20\t \n",
      "HP:0008321\t0.26\t \n",
      "HP:0030269\t0.25\t \n",
      "HP:0001254\t0.05\t \n",
      "HP:0001659\t0.69\t \n",
      "HP:0012506\t0.14\t \n",
      "HP:0001939\t0.35\t \n",
      "HP:0006579\t0.14\t \n",
      "HP:0001169\t0.05\tX\n",
      "HP:0002174\t0.22\t \n",
      "HP:0001102\t0.27\t \n",
      "HP:0001719\t0.64\t \n",
      "HP:0001337\t0.42\t \n",
      "HP:0001647\t0.65\t \n",
      "HP:0002615\t0.53\t \n",
      "HP:0005180\t0.39\t \n",
      "HP:0000605\t0.18\t \n",
      "HP:0001561\t0.10\t \n",
      "HP:0007086\t0.19\t \n",
      "HP:0100753\t0.21\t \n",
      "HP:0100749\t0.87\t \n",
      "HP:0001653\t0.92\t \n",
      "HP:0030682\t0.25\t \n",
      "HP:0003677\t0.25\t \n",
      "HP:0001998\t0.07\t \n",
      "HP:0000787\t0.35\tX\n",
      "HP:0006671\t0.18\t \n",
      "HP:0000988\t0.15\t \n",
      "HP:0002829\t0.36\tX\n",
      "HP:0011858\t0.24\t \n",
      "HP:0000999\t0.11\t \n",
      "HP:0000272\t0.12\tX\n",
      "HP:0000582\t0.04\tX\n",
      "HP:0010546\t0.10\t \n",
      "HP:0000790\t0.06\t \n",
      "HP:0001265\t0.07\t \n",
      "HP:0011663\t0.21\t \n",
      "HP:0004421\t0.98\t \n",
      "HP:0000956\t0.09\t \n",
      "HP:0001763\t0.08\t \n",
      "HP:0002878\t0.95\t \n",
      "HP:0001268\t0.21\t \n",
      "HP:0000044\t0.12\t \n",
      "HP:0004279\t0.06\tX\n",
      "HP:0001639\t0.19\t \n",
      "HP:0000158\t0.10\tX\n",
      "HP:0000256\t0.20\t \n",
      "HP:0012332\t0.23\t \n",
      "HP:0000973\t0.29\t \n",
      "HP:0100867\t0.05\tX\n",
      "HP:0001701\t0.09\t \n",
      "HP:0000099\t0.07\t \n",
      "HP:0002061\t0.25\t \n",
      "HP:0000098\t0.09\t \n",
      "HP:0002215\t0.09\t \n",
      "HP:0000954\t0.06\tX\n",
      "HP:0008202\t0.09\t \n",
      "HP:0001645\t0.94\t \n",
      "HP:0001114\t0.95\t \n",
      "HP:0000474\t0.06\tX\n",
      "HP:0004758\t0.22\t \n",
      "HP:0410030\t0.13\t \n",
      "HP:0000858\t0.26\t \n",
      "HP:0011034\t0.05\t \n",
      "HP:0000458\t0.09\t \n",
      "HP:0000083\t0.37\tX\n",
      "HP:0100520\t0.07\t \n",
      "HP:0002119\t0.18\t \n",
      "HP:0000571\t0.21\t \n",
      "HP:0011960\t0.22\t \n",
      "HP:0030680\t0.09\t \n",
      "HP:0002155\t0.75\t \n",
      "HP:0001903\t0.70\t \n",
      "HP:0010280\t0.06\t \n",
      "HP:0002064\t0.17\t \n",
      "HP:0000246\t0.10\t \n",
      "HP:0002249\t0.25\t \n",
      "HP:0008283\t0.05\t \n",
      "HP:0001274\t0.10\t \n",
      "HP:0100785\t0.11\t \n",
      "HP:0003362\t0.93\t \n",
      "HP:0000819\t0.98\t \n",
      "HP:0001650\t0.65\t \n",
      "HP:0000786\t0.10\t \n",
      "HP:0500001\t0.12\t \n",
      "HP:0001699\t0.90\t \n",
      "HP:0011704\t0.29\t \n",
      "HP:0002362\t0.22\t \n",
      "HP:0003401\t0.17\t \n",
      "HP:0006279\t0.96\t \n",
      "HP:0003676\t0.18\t \n",
      "HP:0009804\t0.12\t \n",
      "HP:0001959\t0.96\t \n",
      "HP:0004963\t0.62\t \n",
      "HP:0003454\t0.05\t \n",
      "HP:0003164\t0.10\t \n",
      "HP:0001260\t0.22\t \n",
      "HP:0006677\t0.24\t \n",
      "HP:0005162\t0.49\t \n",
      "HP:0002035\t0.06\t \n",
      "HP:0001681\t0.90\t \n",
      "HP:0004389\t0.06\t \n",
      "HP:0040278\t0.27\t \n",
      "HP:0000054\t0.12\t \n",
      "HP:0003270\t0.15\t \n",
      "HP:0010550\t0.04\t \n",
      "HP:0004756\t0.91\t \n",
      "HP:0030423\t0.19\t \n",
      "HP:0000016\t0.09\t \n",
      "HP:0000855\t0.94\t \n",
      "HP:0031798\t0.95\t \n",
      "HP:0025059\t0.21\t \n",
      "HP:0001658\t0.95\t \n",
      "HP:0000509\t0.10\t \n",
      "HP:0002181\t0.13\t \n",
      "HP:0012151\t0.10\t \n",
      "HP:0001369\t0.23\t \n",
      "HP:0002019\t0.39\t \n",
      "HP:0008551\t0.06\tX\n",
      "HP:0200118\t0.14\t \n",
      "HP:0003302\t0.03\t \n",
      "HP:0002396\t0.21\t \n",
      "HP:0002063\t0.14\t \n",
      "HP:0002014\t0.21\t \n",
      "HP:0000767\t0.09\t \n",
      "HP:0002592\t0.11\t \n",
      "HP:0001290\t0.12\t \n",
      "HP:0002922\t0.06\t \n",
      "HP:0000013\t0.11\t \n",
      "HP:0000707\t0.05\t \n",
      "HP:0008197\t0.11\t \n",
      "HP:0000739\t0.20\t \n",
      "HP:0000751\t0.21\t \n",
      "HP:0001973\t0.05\t \n",
      "HP:0001176\t0.16\t \n",
      "HP:0001269\t0.05\t \n",
      "HP:0007256\t0.26\t \n",
      "HP:0003729\t0.10\t \n",
      "HP:0002023\t0.07\tX\n",
      "HP:0003487\t0.22\t \n",
      "HP:0002633\t0.05\t \n",
      "HP:0100754\t0.16\t \n",
      "HP:0003338\t0.23\t \n",
      "HP:0100819\t0.06\t \n",
      "HP:0001412\t0.09\t \n",
      "HP:0002650\t0.04\t \n",
      "HP:0002171\t0.18\t \n",
      "HP:0002251\t0.13\tX\n",
      "HP:0000463\t0.08\t \n",
      "HP:0012223\t0.22\t \n",
      "HP:0100660\t0.46\t \n",
      "HP:0002465\t0.23\t \n",
      "HP:0003467\t0.07\tX\n",
      "HP:0200123\t0.05\t \n",
      "HP:0001915\t0.09\t \n",
      "HP:0000725\t0.24\t \n",
      "HP:0000271\t0.09\t \n",
      "HP:0002715\t0.13\t \n",
      "HP:0003584\t0.96\t \n",
      "HP:0011800\t0.11\t \n",
      "HP:0012248\t0.63\t \n",
      "HP:0025143\t0.05\t \n",
      "HP:0002173\t0.06\t \n",
      "HP:0004322\t0.08\tX\n",
      "HP:0002751\t0.08\t \n",
      "HP:0040171\t0.09\t \n",
      "HP:0008724\t0.11\t \n",
      "HP:0001621\t0.25\t \n",
      "HP:0030084\t0.13\t \n",
      "HP:0000076\t0.10\t \n",
      "HP:0031295\t0.16\t \n",
      "HP:0012154\t0.14\t \n",
      "HP:0011463\t0.13\t \n",
      "HP:0030341\t0.13\t \n",
      "HP:0000871\t0.09\t \n",
      "HP:0001945\t0.13\t \n",
      "HP:0010538\t0.07\t \n",
      "HP:0001279\t0.91\t \n",
      "HP:0410054\t0.25\t \n",
      "HP:0011712\t0.61\t \n",
      "HP:0002248\t0.21\t \n",
      "HP:0002893\t0.26\t \n",
      "HP:0011755\t0.08\t \n",
      "HP:0001662\t0.91\t \n",
      "HP:0000286\t0.07\tX\n",
      "HP:0002225\t0.09\t \n",
      "HP:0000802\t0.07\t \n",
      "HP:0000823\t0.08\t \n",
      "HP:0001341\t0.11\t \n",
      "HP:0100543\t0.19\t \n",
      "HP:0008843\t0.21\t \n",
      "HP:0001395\t0.07\t \n",
      "HP:0000324\t0.10\t \n",
      "HP:0000010\t0.11\t \n",
      "HP:0001943\t0.12\t \n",
      "HP:0003140\t0.19\t \n",
      "HP:0002611\t0.05\t \n",
      "HP:0000518\t0.97\t \n",
      "HP:0002120\t0.21\t \n",
      "HP:0012185\t0.06\t \n",
      "HP:0002607\t0.05\t \n",
      "HP:0002027\t0.11\t \n",
      "HP:0002529\t0.23\t \n",
      "HP:0011710\t0.80\t \n",
      "HP:0030344\t0.08\t \n",
      "HP:0001718\t0.67\t \n",
      "HP:0100315\t0.22\t \n",
      "HP:0002718\t0.10\t \n",
      "HP:0012232\t0.65\t \n",
      "HP:0002401\t0.04\t \n",
      "HP:0000969\t0.10\t \n",
      "HP:0001680\t0.64\t \n",
      "HP:0000020\t0.06\t \n",
      "HP:0031047\t0.06\t \n",
      "HP:0011919\t0.24\t \n",
      "HP:0004926\t0.05\t \n",
      "HP:0001663\t0.91\t \n",
      "HP:0002304\t0.16\t \n",
      "HP:0000643\t0.23\t \n",
      "HP:0003124\t0.95\t \n",
      "HP:0000453\t0.14\t \n",
      "HP:0002590\t0.14\t \n",
      "HP:0000821\t0.07\tX\n",
      "HP:0003149\t0.37\tX\n",
      "HP:0000027\t0.09\t \n",
      "HP:0005144\t0.20\t \n",
      "HP:0000845\t0.22\t \n",
      "HP:0002750\t0.17\t \n",
      "HP:0005117\t0.97\t \n",
      "HP:0031258\t0.57\t \n",
      "HP:0032192\t0.80\t \n",
      "HP:0000029\t0.12\t \n",
      "HP:0001638\t0.48\t \n",
      "HP:0000298\t0.19\t \n",
      "HP:0000365\t0.10\t \n",
      "HP:0011220\t0.11\t \n",
      "HP:0031274\t0.16\t \n",
      "HP:0005547\t0.06\tX\n",
      "HP:0001252\t0.09\tX\n",
      "HP:0000825\t0.05\t \n",
      "HP:0011760\t0.29\t \n",
      "HP:0012368\t0.06\tX\n",
      "HP:0002925\t0.13\t \n",
      "HP:0001714\t0.17\t \n",
      "HP:0003782\t0.09\t \n",
      "HP:0003139\t0.11\t \n",
      "HP:0000639\t0.07\t \n",
      "HP:0002548\t0.24\t \n",
      "HP:0002185\t0.47\t \n",
      "HP:0002423\t0.16\t \n",
      "HP:0000822\t0.97\t \n",
      "HP:0002357\t0.12\t \n",
      "HP:0000938\t0.14\t \n",
      "HP:0100829\t0.28\t \n",
      "HP:0200119\t0.07\t \n",
      "HP:0001300\t0.50\t \n",
      "HP:0030151\t0.09\t \n",
      "HP:0031972\t0.78\t \n",
      "HP:0002591\t0.97\t \n",
      "HP:0001271\t0.06\t \n",
      "HP:0031959\t0.25\t \n",
      "HP:0001824\t0.22\t \n",
      "HP:0012828\t0.13\t \n",
      "HP:0000851\t0.13\t \n",
      "HP:0003304\t0.04\t \n",
      "HP:0000839\t0.08\t \n",
      "HP:0002013\t0.13\t \n",
      "HP:0001962\t0.80\t \n",
      "HP:0002617\t0.21\t \n",
      "HP:0004972\t0.98\t \n",
      "HP:0001325\t0.06\t \n",
      "HP:0002090\t0.94\t \n",
      "HP:0000012\t0.20\t \n",
      "HP:0001635\t0.96\t \n",
      "HP:0012340\t0.13\t \n",
      "HP:0000164\t0.09\t \n",
      "HP:0001634\t0.23\t \n",
      "HP:0000338\t0.24\t \n",
      "HP:0002451\t0.18\t \n",
      "HP:0002322\t0.15\t \n",
      "HP:0000093\t0.13\t \n",
      "HP:0001649\t0.63\t \n",
      "HP:0000421\t0.23\t \n",
      "HP:0001969\t0.05\t \n",
      "HP:0002094\t0.84\t \n",
      "HP:0005353\t0.04\t \n",
      "HP:0005155\t0.22\t \n",
      "HP:0002149\t0.44\tX\n",
      "HP:0001336\t0.22\t \n",
      "HP:0100502\t0.09\t \n",
      "HP:0011999\t0.18\t \n",
      "HP:0012272\t0.65\t \n",
      "HP:0001678\t0.20\t \n",
      "HP:0000122\t0.09\t \n",
      "HP:0003298\t0.08\t \n",
      "HP:0001640\t0.08\t \n",
      "HP:0002375\t0.21\t \n",
      "HP:0001889\t0.11\t \n",
      "HP:0001257\t0.22\t \n",
      "HP:0000831\t0.94\t \n",
      "HP:0000514\t0.18\t \n",
      "HP:0001629\t0.67\t \n",
      "HP:0031179\t0.06\t \n",
      "HP:0006775\t0.03\t \n",
      "HP:0007430\t0.07\t \n",
      "HP:0040270\t0.81\t \n",
      "HP:0002321\t0.51\t \n",
      "HP:0011920\t0.07\t \n",
      "HP:0100806\t0.94\t \n",
      "HP:0001324\t0.06\t \n",
      "HP:0000726\t0.60\t \n",
      "HP:0001997\t0.87\tX\n",
      "HP:0030339\t0.09\t \n",
      "HP:0003587\t0.18\t \n",
      "HP:0001833\t0.16\t \n",
      "HP:0000789\t0.12\t \n",
      "HP:0012416\t0.95\t \n",
      "HP:0000842\t0.04\t \n",
      "HP:0000612\t0.07\t \n",
      "HP:0002071\t0.22\t \n",
      "HP:0001298\t0.08\t \n",
      "HP:0000738\t0.94\t \n",
      "HP:0002007\t0.33\t \n",
      "HP:0031800\t0.93\t \n",
      "HP:0002383\t0.12\t \n",
      "HP:0002315\t0.06\t \n",
      "HP:0000280\t0.24\t \n",
      "HP:0006733\t0.06\tX\n"
     ]
    }
   ],
   "source": [
    "nn_files.test(model,loss_func=loss_func_nn, device=device, dataset_split =dataset_split,\n",
    "              target_data_creator=target_data_creator, real_effect=real_effect, calc_accuracy=calc_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8576/1787606000.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mAE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'encoder'"
     ]
    }
   ],
   "source": [
    "AE.encoder == model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "daceadcb7888934c6f7a76168830572155d751a6b865934a713a3e2629d639d6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
