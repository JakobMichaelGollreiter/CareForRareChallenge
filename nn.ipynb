{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Notebook\n",
    "\n",
    "Dear User,\n",
    "\n",
    "this Jupyter Notebook contains the code and AI model to our challenge \"Rare Diseases\" by the LMU childrens hospital.\n",
    "Here our various functions come together to create a data pipeline and readable as well as modular model architecture.\n",
    "There are few medical terms being used whose understanding is necessary and they are the following:\n",
    "\n",
    "`HPO Features`: Human Phenotype Ontology Features provide a standardized vocabulary of phenotypic abnormalities encountered in human disease.\n",
    "`ICD-9`: International standard for the classification of diseases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nn_data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import nn_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Labevents_HPO are the HPO features which where measured during an examination of a patient\n",
    "# Diagnoses_HPO are the HPO features that were diagnosed based\n",
    "HPO_PATH = 'data/hp.obo'\n",
    "LABEVENTS_HPO_PATH = 'data/OUT_LABEVENTS_HPO.csv'\n",
    "DIAGNOSES_HPO_PATH = 'data/DIAGNOSE_ICD_hpo.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads HPO data, labevents and diagnoses and groups them by subject ID\n",
    "data = nn_data.LoadedData(HPO_PATH, LABEVENTS_HPO_PATH, DIAGNOSES_HPO_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# splits the loaded data in labevents  and diagnoses\n",
    "input_data_creator = nn_data.HPODatasetCreator(\n",
    "    data, mode='labevents', enable_parent_nodes=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set this to False or True depending if you want use HPO features as output(False) or ICD codes(True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "use_ICD = False\n",
    "\n",
    "if not use_ICD:\n",
    "    target_data_creator = nn_data.HPODatasetCreator(\n",
    "        data, mode='diagnoses', enable_parent_nodes=True)\n",
    "else:\n",
    "    target_data_creator = nn_data.ICDDatasetCreator(data, batch = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data from the input and target_data_creators are transferred in a nested list structure\n",
    "input_data: list[list[int]] = input_data_creator.data()\n",
    "target_data: list[list[int]] = target_data_creator.data()\n",
    "\n",
    "# for pytorch the list structure is transformed in tensors\n",
    "input_tensor = torch.FloatTensor(input_data)\n",
    "target_tensor = torch.FloatTensor(target_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(output, target) -> float:\n",
    "    \"\"\"\n",
    "    definition of an accuracy function\n",
    "    \"\"\"\n",
    "    number_of_features = target.sum(axis=1)\n",
    "    correctly_identified = (target * np.sqrt(output)).sum(axis=1)\n",
    "    return np.mean(correctly_identified / (number_of_features + .00001))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_effect(outputs, targets):\n",
    "    \"\"\"\n",
    "    lists the correctly identified diagnoses, false positives and false negatives\n",
    "    correct diagnoses counts only recognized active features and not neglected inactive features\n",
    "    \"\"\" \n",
    "    correct_diagnosed = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    total_to_diagnose = sum(targets[0])\n",
    "\n",
    "    for i in range(len(outputs[0])):\n",
    "        if(outputs[0, i] >= 0.5 and targets[0, i] == 1):\n",
    "            correct_diagnosed += 1\n",
    "        if(outputs[0, i] < 0.5 and targets[0, i] == 1):\n",
    "            false_negative += 1\n",
    "        if(outputs[0, i] > 0.5 and targets[0, i] == 0):\n",
    "            false_positive += 1\n",
    "\n",
    "    print(\"Correct diagnoses:\" f'{correct_diagnosed}/{total_to_diagnose}')\n",
    "    print(\"False positives:\" f'{false_positive}')\n",
    "    print(\"False negatives:\" f'{false_negative}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device selection, where NN is trained\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage of Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True or False\n",
    "use_autoencoder = True  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_autoencoder:\n",
    "    \n",
    "    # Reduction factors symbolize how stronly the data is compressed in the latent space\n",
    "    reduction_factor_hidden = 0.7\n",
    "    reduction_factor_latent = 0.5\n",
    "\n",
    "    # Dimensions of the Autoencoder\n",
    "    input_size_AE = len(input_data[0])\n",
    "    hidden_size_AE = int(input_size_AE*reduction_factor_hidden)\n",
    "    latent_size_AE = int(input_size_AE*reduction_factor_latent)\n",
    "\n",
    "    # definition of loss function\n",
    "    # usefullness of CrossEntropyLoss for Model and Autoencoder Training \n",
    "    # determined experimentally, best among available pytorch loss functions\n",
    "    loss_func_AE = nn.MSELoss()\n",
    "    loss_func_NN = nn.CrossEntropyLoss()\n",
    "\n",
    "    # In Autoencoder funciton the architecture is built\n",
    "    AE = nn_files.Autoencoder(input_size_AE, hidden_size_AE, latent_size_AE)\n",
    "    AE.to(device)\n",
    "\n",
    "    # Set enlarging factor for the following FCN (Fully Connected Network)\n",
    "    enlarging_factor_NN = 1.4\n",
    "    \n",
    "    # Latent space of autoencoder is used as input for the FCN\n",
    "    input_size_NN = latent_size_AE\n",
    "else:\n",
    "    AE = None\n",
    "\n",
    "    loss_func_NN = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Enlarging factor and input size of the FCN if no encoder is used\n",
    "    enlarging_factor_NN = 1.4\n",
    "    input_size_NN = len(input_data[0])\n",
    "\n",
    "output_size_NN = len(target_data[0])\n",
    "hidden_size_NN = int(max(input_size_NN, output_size_NN) * enlarging_factor_NN)\n",
    "\n",
    "# Call of NN function, can build Model differently depending on if encoder is used or not\n",
    "model = nn_files.FCNModel(input_size_NN, hidden_size_NN,\n",
    "                    output_size_NN, enlarging_factor_NN, AE, dropOutRatio=0)\n",
    "_ = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with Training Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if use_autoencoder:\n",
    "    \n",
    "    # set the parameters for the training of the autoencoder\n",
    "    batch_size = 8\n",
    "    learning_rate = 1e-2\n",
    "    num_epochs = 60\n",
    "\n",
    "    # Bestimmung des Optimizers, standard: Adam\n",
    "    optimizer = torch.optim.Adam(\n",
    "        AE.parameters(),\n",
    "        lr=learning_rate,\n",
    "        betas=(0.9, 0.999),\n",
    "    )\n",
    "\n",
    "    # create seperate dataset for Autoencoder, as output of model is not compared to orignal target\n",
    "    # but again to the input\n",
    "    dataset_AE = torch.utils.data.TensorDataset(input_tensor, input_tensor)\n",
    "    dataset_AE_split = nn_files.split_dataset(batch_size, dataset_AE)\n",
    "\n",
    "    # train autoencoder with general training funciton, can handle autoencoder and other models\n",
    "    nn_files.training(\n",
    "        AE, device, dataset_AE_split,\n",
    "        optimizer=optimizer, loss_func=loss_func_AE,  num_epochs=num_epochs,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_autoencoder:\n",
    "    \n",
    "    # show the output of the autoencoder to visualize ability to decode and reconstruct multi-hot encoding vectors\n",
    "    # X visualize an active feature, blanks a zero\n",
    "    nn_files.test(\n",
    "        AE, device, dataset_AE_split, data_creator=input_data_creator,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_autoencoder:\n",
    "    \n",
    "    # parameters for training of FCN with pretrained encoder\n",
    "    batch_size = 8\n",
    "    learning_rate = 1e-4\n",
    "    num_epochs = 20\n",
    "\n",
    "    # Bestimmung des Optimizers, standard: Adam\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        betas=(0.9, 0.999),\n",
    "    )\n",
    "else:\n",
    "    \n",
    "    # parameters for training of FCN without pretrained encoder\n",
    "    batch_size = 8\n",
    "    learning_rate = 1e-4\n",
    "    num_epochs = 20\n",
    "\n",
    "    # Bestimmung des Optimizers, standard: Adam\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        betas=(0.9, 0.999),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset in which the input and target aka fround truth tensor are located enxt to each other\n",
    "dataset = torch.utils.data.TensorDataset(input_tensor, target_tensor)\n",
    "\n",
    "# split the dataset in 70% training data, 20% validation data, 10% test data\n",
    "dataset_split = nn_files.split_dataset(batch_size, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model, log_rhythm denotes after how many iterations in one epoch \n",
    "# the loss and accuracy if applicable is printe\n",
    "nn_files.training(\n",
    "    model, device, dataset_split,\n",
    "    num_epochs=num_epochs,\n",
    "    optimizer=optimizer, loss_func=loss_func_NN,\n",
    "    real_effect=real_effect, calc_accuracy=calc_accuracy,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the real effect of the prediction, aka understandable by humans in a way of classification in\n",
    "# correct diagnoses, false positives and false negatives\n",
    "nn_files.test(\n",
    "    model, device, dataset_split,\n",
    "    target_data_creator,\n",
    "    real_effect=real_effect, calc_accuracy=calc_accuracy,\n",
    "    sort_output_by_confidence=True,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "daceadcb7888934c6f7a76168830572155d751a6b865934a713a3e2629d639d6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
